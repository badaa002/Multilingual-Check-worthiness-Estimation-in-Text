{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client, Options\n",
    "from typing import Dict, Any, List\n",
    "import yaml\n",
    "import re\n",
    "import os\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "CHECKWORTHY_PROMPT = \"\"\"Given a sentence from a political debate or tweet, your task is to predict if it is fact check-worthy statement or not.\n",
    " Your response must always start with \"My response is: (yes or no based on your prediction)\"\n",
    " your prediction must be yes or no, and i wont accept you prediction to be \"I cannot determine\"\n",
    " Never write \"I'm and AI language model...\"\n",
    " Examples:\n",
    " fact check-worthy: The sky is blue.\n",
    " Not fact check-worthy: I like ice cream.\n",
    " fact check-worthy: The distance between the earth and the moon is 238,855 km.\n",
    " Not fact check-worthy: It made the best ratings in the history of television.\n",
    " fact check-worthy: The unemployment rate is 5.2% in Norway.\n",
    " Not fact check-worthy: I am the best president in the history of the United States.\n",
    "Sentence: {claim}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ollama:\n",
    "    \"\"\"A class for generating questions and interacting with the Ollama API.\"\"\"\n",
    "\n",
    "    def __init__(self, config_path: str = \"mistral.yaml\"):\n",
    "        \"\"\"Initializes the Ollama client and loads necessary configurations.\"\"\"\n",
    "        self._ollama_client = Client(timeout=20)\n",
    "        self._config_path = config_path\n",
    "        self._config = self._load_config()\n",
    "        self._stream = self._config.get(\"stream\", False)\n",
    "        self._model_name = self._config.get(\"model\", \"mistral\")\n",
    "        self._llm_options = self._get_llm_config()\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate text using Ollama LLM for the given prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt: Prompt for the LLM.\n",
    "\n",
    "        Returns:\n",
    "            Response text from an Ollama LLM.\n",
    "        \"\"\"\n",
    "        response = self._ollama_client.generate(\n",
    "            model=self._model_name,\n",
    "            prompt=prompt,\n",
    "            options=self._llm_options,\n",
    "            stream=self._stream,\n",
    "        )\n",
    "        return response.get(\"response\", \"\").strip()  # type: ignore\n",
    "\n",
    "    def _load_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Loads configuration from a YAML file.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the config file is not found.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with configuration values.\n",
    "        \"\"\"\n",
    "        if not os.path.isfile(self._config_path):\n",
    "            raise FileNotFoundError(f\"Config file {self._config_path} not found.\")\n",
    "        with open(self._config_path, \"r\") as file:\n",
    "            yaml_data = yaml.safe_load(file)\n",
    "        return yaml_data\n",
    "\n",
    "    def _get_llm_config(self) -> Options:\n",
    "        \"\"\"Extracts and returns the LLM (language learning model) configuration.\n",
    "\n",
    "        Returns:\n",
    "            An Options object with the LLM configuration.\n",
    "        \"\"\"\n",
    "        return Options(self._config.get(\"options\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = Ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 318/318 [20:24<00:00,  3.85s/it]   \n"
     ]
    }
   ],
   "source": [
    "listofpredicts = list()\n",
    "df = pd.read_csv(\n",
    "    \"../data/processed/processed_CT24_checkworthy_english/processed_dev_test.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    response = ollama.generate(CHECKWORTHY_PROMPT.format(claim=row[\"text\"]))\n",
    "    response_words = response.split()\n",
    "    answer = response_words[3].strip(\".,\").lower()\n",
    "    if answer == \"yes\" or answer == \"no\":\n",
    "        listofpredicts.append(response_words[3].strip(\".,\").lower())\n",
    "    else:\n",
    "        listofpredicts.append(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'no', 'no', 'no', 'no']\n"
     ]
    }
   ],
   "source": [
    "print(listofpredicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8450755940916146\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(df[\"class_label\"].str.lower(), listofpredicts, average=\"weighted\")\n",
    "print(\"F1 Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
