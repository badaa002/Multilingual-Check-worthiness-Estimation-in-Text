{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca1YIq9zb3KP",
        "outputId": "5045f923-5228-48d9-c82c-77e858de1949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijf3uJO2Rq6l",
        "outputId": "5e86d60a-1629-44b0-962d-9cefa0106f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7333, 4)\n",
            "Class Distribution:\n",
            "class_label\n",
            "No     69.412246\n",
            "Yes    30.587754\n",
            "Name: proportion, dtype: float64\n",
            "Missing values:\n",
            "tweet_id       0\n",
            "tweet_url      0\n",
            "tweet_text     0\n",
            "class_label    0\n",
            "text_length    0\n",
            "dtype: int64\n",
            "Number of duplicate rows: 0\n",
            "Number of Arabic tweets: 7333\n",
            "Number of tweets with URL: 4845\n",
            "Number of tweets with noise: 5174\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "def contains_arabic(text):\n",
        "    arabic_pattern = re.compile('[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]+')\n",
        "    return bool(arabic_pattern.search(text))\n",
        "\n",
        "def contains_url(text):\n",
        "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return bool(url_pattern.search(text))\n",
        "\n",
        "# Function to analyze sentiment using VADER\n",
        "def analyze_sentiment_vader(text):\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']\n",
        "\n",
        "# Function to categorize sentiment score\n",
        "def categorize_sentiment(score):\n",
        "    bins = [-1, -0.25, 0, 0.25, 0.5, 0.75, 1]\n",
        "    labels = ['very_negative', 'negative', 'neutral', 'positive', 'very_positive', 'extremely_positive']\n",
        "    return pd.cut([score], bins=bins, labels=labels)[0]\n",
        "\n",
        "def symmentic_text(text):\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "    return text\n",
        "\n",
        "\n",
        "def contains_noise(text):\n",
        "    # Check for short length\n",
        "    if len(text.split()) < 4:\n",
        "        return True\n",
        "\n",
        "    # Check for repetitive characters\n",
        "    if re.search(r'(.)\\1{2,}', text):\n",
        "        return True\n",
        "\n",
        "    # Check for non-Arabic characters (assuming Arabic text is in Unicode range)\n",
        "    if not re.search(r'[\\u0600-\\u06FF]', text):\n",
        "        return True\n",
        "\n",
        "    # Check for excessive punctuation\n",
        "    if re.search(r'[!?.]{4,}', text):\n",
        "        return True\n",
        "\n",
        "    # Check for excessive numbers\n",
        "    if re.search(r'\\d{5,}', text):\n",
        "        return True\n",
        "\n",
        "    # Check for URLs\n",
        "    if re.search(r'http\\S+|www.\\S+', text):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# Exclude Arabic characters\n",
        "def check_non_ascii_tweets(df):\n",
        "    non_ascii_df = df[df['tweet_text'].apply(lambda x: any((ord(char) > 127 and ord(char) < 1536 and not '\\u0600' <= char <= '\\u06FF') for char in x))]\n",
        "    return non_ascii_df\n",
        "\n",
        "\n",
        "def data_exploration(train_df):\n",
        "  print (train_df.shape)\n",
        "  class_distribution = train_df['class_label'].value_counts(normalize=True) * 100\n",
        "  print(\"Class Distribution:\")\n",
        "  print(class_distribution)\n",
        "\n",
        "  train_df['text_length'] = train_df['tweet_text'].apply(len)\n",
        "  missing_values = train_df.isnull().sum()\n",
        "  print(\"Missing values:\")\n",
        "  print(missing_values)\n",
        "  duplicate_rows = train_df.duplicated().sum()\n",
        "  print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
        "\n",
        "  arabic_tweets = train_df[train_df['tweet_text'].apply(contains_arabic)]\n",
        "  print(f\"Number of Arabic tweets: {len(arabic_tweets)}\")\n",
        "\n",
        "  tweets_with_url = train_df[train_df['tweet_text'].apply(contains_url)]\n",
        "  print(f\"Number of tweets with URL: {len(tweets_with_url)}\")\n",
        "\n",
        "  tweets_with_noise = train_df[train_df['tweet_text'].apply(contains_noise)]\n",
        "  print(f\"Number of tweets with noise: {len(tweets_with_noise)}\")\n",
        "\n",
        "  non_ascii_df = check_non_ascii_tweets(train_df)\n",
        "  print(f\"Number of tweets with non-ASCII characters: {len(non_ascii_df)}\\n\")\n",
        "  print(\"Examples:\")\n",
        "  print(non_ascii_df['tweet_text'].head())\n",
        "\n",
        "\n",
        "  # Apply preprocessing for sentiment\n",
        "  train_df['clean_text'] = train_df['tweet_text'].apply(symmentic_text)\n",
        "\n",
        "  # Analyze sentiment using VADER\n",
        "  train_df['sentiment'] = train_df['clean_text'].apply(analyze_sentiment_vader)\n",
        "\n",
        "  # Hashtag and Mention Analysis\n",
        "  train_df['hashtags'] = train_df['tweet_text'].apply(lambda x: [word[1:] for word in x.split() if word.startswith('#')])\n",
        "  print(\"Most Common Hashtags:\\n\", pd.Series([item for sublist in train_df['hashtags'] for item in sublist]).value_counts().head(10))\n",
        "\n",
        "  train_df['mentions'] = train_df['tweet_text'].apply(lambda x: [word[1:] for word in x.split() if word.startswith('@')])\n",
        "  print(\"Most Common Mentions:\\n\", pd.Series([item for sublist in train_df['mentions'] for item in sublist]).value_counts().head(10))\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "def load(filename):\n",
        "    df = pd.read_csv(filename, sep='\\t', encoding='utf-8', names=['tweet_id', 'tweet_url', 'tweet_text', 'class_label'], quoting=csv.QUOTE_NONE, skiprows=1, dtype={'tweet_id': 'Int64'})\n",
        "    data_exploration(df)\n",
        "    df.drop(columns=['tweet_url'], inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "train_df = load('CT24_checkworthy_arabic_train.tsv')\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_adHGkNyLDK"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "\n",
        "\n",
        "Amb. words:\n",
        "    أها (Aha)\n",
        "    آه (Ah)\n",
        "    يع (Yeah)\n",
        "    همم (Hm)\n",
        "    ههه (Haha, similar to \"lol\" in English)\n",
        "    هوو (Hoo)\n",
        "\n",
        "    Remove URLs: remove_urls\n",
        "    Replace Repeated Special Characters: replace_repeated_characters\n",
        "    Replace Commas and Double Quotes: replace_commas_and_quotes\n",
        "    Remove Ambiguous Words: remove_ambiguous_words\n",
        "    Remove Non-ASCII Characters: remove_non_ascii\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKn_lQ0vHXEq"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "def remove_urls(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "    # Fix user\n",
        "    text = re.sub(r'(@\\w+\\s*)+', '@<USER> ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    # Remove non-ASCII characters except Arabic script\n",
        "    # Keep Arabic script characters while removing other non-ASCII characters\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\x00-\\x7F]+', ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def replace_repeated_characters(text):\n",
        "    # Replace repeated special characters with a single occurrence\n",
        "    text = re.sub(r'([!?]){2,}', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def replace_commas_and_quotes(text):\n",
        "    # Replace ،، with a single comma ,\n",
        "    text = text.replace('،،', ',')\n",
        "\n",
        "    # Replace multiple double quotes with a single occurrence\n",
        "    text = re.sub(r'\"{2,}', '\"', text)\n",
        "    return text\n",
        "\n",
        "def remove_ambiguous_words(text):\n",
        "    ambiguous_words = ['أها', 'آه', 'يع', 'ههه', 'همم', 'هوو']\n",
        "\n",
        "    for word in ambiguous_words:\n",
        "        # Remove the ambiguous word and fix spaces\n",
        "        text = re.sub(r'\\b{}\\b'.format(word), '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # Fix extra spaces\n",
        "\n",
        "        # Remove comma or punctuation on the left side of the removed word\n",
        "        text = re.sub(r',?\\s*{}'.format(word), '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocessing(train_df):\n",
        "\n",
        "    train_df['tweet_text'] = train_df['tweet_text'].apply(remove_urls)\n",
        "    train_df['tweet_text'] = train_df['tweet_text'].apply(replace_commas_and_quotes)\n",
        "    train_df['tweet_text'] = train_df['tweet_text'].apply(replace_repeated_characters)\n",
        "    train_df['tweet_text'] = train_df['tweet_text'].apply(remove_ambiguous_words)\n",
        "    train_df['tweet_text'] = train_df['tweet_text'].apply(remove_non_ascii)\n",
        "\n",
        "    # Sentiment polarity category\n",
        "    train_df['sentiment_category'] = train_df['sentiment'].apply(categorize_sentiment)\n",
        "\n",
        "\n",
        "    # Calculate text length\n",
        "    train_df['text_length'] = train_df['tweet_text'].apply(len)\n",
        "\n",
        "    # Categorize text length\n",
        "    bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, np.inf]\n",
        "    labels = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '100+']\n",
        "    train_df['text_length_category'] = pd.cut(train_df['text_length'], bins=bins, labels=labels)\n",
        "\n",
        "    # Label encode class label\n",
        "    label_encoder = LabelEncoder()\n",
        "    train_df['class_label_encoded'] = label_encoder.fit_transform(train_df['class_label'])\n",
        "\n",
        "\n",
        "    train_df = train_df.drop(columns=['text_length', 'sentiment', 'clean_text'])\n",
        "\n",
        "    #data_exploration(train_df)\n",
        "    return train_df\n",
        "\n",
        "train_df = preprocessing(train_df)\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG1jjrGmZxQa"
      },
      "source": [
        "## Feature engineering\n",
        "  * Add Frequency of Hashtags\n",
        "  * Sentiment Analysis of Hashtags\n",
        "  * Topic Modeling with LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZlLlTeQZyG8"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download Arabic stop words\n",
        "nltk.download('stopwords')\n",
        "arabic_stop_words = list(stopwords.words('arabic'))\n",
        "\n",
        "# Function to count frequency of hashtags\n",
        "def count_hashtags_frequency(hashtags_list):\n",
        "    return len(hashtags_list)\n",
        "\n",
        "# Function to analyze sentiment of hashtags\n",
        "def analyze_hashtag_sentiment(hashtags_list):\n",
        "    if hashtags_list:\n",
        "        sentiment_scores = [TextBlob(hashtag).sentiment.polarity for hashtag in hashtags_list]\n",
        "        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
        "        return avg_sentiment\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function for topic modeling with LDA\n",
        "def topic_modeling_with_lda(texts):\n",
        "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=arabic_stop_words)\n",
        "    dtm = vectorizer.fit_transform(texts)\n",
        "\n",
        "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "    lda.fit(dtm)\n",
        "\n",
        "    topics = lda.transform(dtm)\n",
        "\n",
        "    return topics.argmax(axis=1)\n",
        "\n",
        "\n",
        "def add_additional_features(df):\n",
        "    # Count frequency of hashtags\n",
        "    df['hashtags_frequency'] = df['hashtags'].apply(count_hashtags_frequency)\n",
        "\n",
        "    # Analyze sentiment of hashtags\n",
        "    df['hashtags_sentiment'] = df['hashtags'].apply(analyze_hashtag_sentiment)\n",
        "\n",
        "    # Topic modeling with LDA\n",
        "    df['hashtags_topics'] = topic_modeling_with_lda(df['tweet_text'])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "train_df = add_additional_features(train_df)\n",
        "train_df.head(4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYXmHbGL47XN"
      },
      "source": [
        "### Preprocessing dev and dev_test and saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQO9-s5t4iqb"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def preprocess_dev_data(filename):\n",
        "    df = load(filename)\n",
        "    df = preprocessing(df)\n",
        "    df = add_additional_features(df)\n",
        "    return df\n",
        "\n",
        "def save_processed_dev_data(df, filepath):\n",
        "    df.to_csv(filepath, sep='\\t', index=False, quoting=csv.QUOTE_NONE)\n",
        "\n",
        "#'CT24_checkworthy_arabic/CT24_checkworthy_arabic_dev.tsv'\n",
        "dev_df = preprocess_dev_data('CT24_checkworthy_arabic_dev.tsv')\n",
        "#'CT24_checkworthy_arabic/CT24_checkworthy_arabic_dev-test.tsv'\n",
        "dev_test_df = preprocess_dev_data('CT24_checkworthy_arabic_dev-test.tsv')\n",
        "\n",
        "save_processed_dev_data(dev_df, 'processed_arabic_dev.tsv')\n",
        "save_processed_dev_data(dev_test_df, 'processed_arabic_dev_test.tsv')\n",
        "save_processed_dev_data(train_df, 'processed_arabic_train.tsv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}