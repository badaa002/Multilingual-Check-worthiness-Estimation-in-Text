{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = \"../data/processed/processed_CT24_checkworthy_english/processed_train.tsv\"\n",
    "du_train = \"../data/processed/processed_CT24_checkworthy_dutch/processed_dutch_train.tsv\"  # noqa\n",
    "es_train = \"../data/processed/processed_CT24_checkworthy_spanish/processed_spanish_train.tsv\"  # noqa\n",
    "ar_train = \"../data/processed/processed_CT24_checkworthy_arabic/processed_arabic_train.tsv\"  # noqa\n",
    "\n",
    "en_test = \"../data/processed/processed_CT24_checkworthy_english/processed_dev.tsv\"\n",
    "du_test = \"../data/processed/processed_CT24_checkworthy_dutch/processed_dutch_dev.tsv\"\n",
    "es_test = \"../data/processed/processed_CT24_checkworthy_spanish/processed_spanish_dev.tsv\"  # noqa\n",
    "ar_test = \"../data/processed/processed_CT24_checkworthy_arabic/processed_arabic_dev.tsv\"\n",
    "\n",
    "en_dev_test = \"../data/processed/processed_CT24_checkworthy_english/processed_dev_test.tsv\"  # noqa\n",
    "du_dev_test = \"../data/processed/processed_CT24_checkworthy_dutch/processed_dutch_dev_test.tsv\"  # noqa\n",
    "es_dev_test = \"../data/processed/processed_CT24_checkworthy_spanish/processed_spanish_dev_test.tsv\"  # noqa\n",
    "ar_dev_test = \"../data/processed/processed_CT24_checkworthy_arabic/processed_arabic_dev_test.tsv\"  # noqa\n",
    "\n",
    "en_train = pd.read_csv(en_train, sep=\"\\t\")\n",
    "du_train = pd.read_csv(du_train, sep=\"\\t\")\n",
    "es_train = pd.read_csv(es_train, sep=\"\\t\")\n",
    "ar_train = pd.read_csv(ar_train, sep=\"\\t\")\n",
    "\n",
    "en_test = pd.read_csv(en_test, sep=\"\\t\")\n",
    "du_test = pd.read_csv(du_test, sep=\"\\t\")\n",
    "es_test = pd.read_csv(es_test, sep=\"\\t\")\n",
    "ar_test = pd.read_csv(ar_test, sep=\"\\t\")\n",
    "\n",
    "en_dev_test = pd.read_csv(en_dev_test, sep=\"\\t\")\n",
    "du_dev_test = pd.read_csv(du_dev_test, sep=\"\\t\")\n",
    "es_dev_test = pd.read_csv(es_dev_test, sep=\"\\t\")\n",
    "ar_dev_test = pd.read_csv(ar_dev_test, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>class_label</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_length_category</th>\n",
       "      <th>class_label_encoded</th>\n",
       "      <th>hashtags_frequency</th>\n",
       "      <th>hashtags_sentiment</th>\n",
       "      <th>hashtags_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>You know, I saw a movie - Crocodile Dundee.\"</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46</td>\n",
       "      <td>41-50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>We're consuming 50 percent of the world's coca...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>41-50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>That answer was about as clear as Boston harbor.</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>41-50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131</td>\n",
       "      <td>Let me help the governor.</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>21-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>We've run up more debt in the last eight years...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125</td>\n",
       "      <td>100+</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>28958</td>\n",
       "      <td>He has promised a trillion dollars out of the ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130</td>\n",
       "      <td>100+</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>28965</td>\n",
       "      <td>(LAUGHTER) I -- there's an old high school deb...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131</td>\n",
       "      <td>100+</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>29011</td>\n",
       "      <td>Well, can I answer that?</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>21-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>29060</td>\n",
       "      <td>I look forward to the final weeks of this camp...</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>51-60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>29062</td>\n",
       "      <td>For those of you for me, thanks for your help.</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>41-50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1032 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence_id                                         tweet_text  \\\n",
       "0              26       You know, I saw a movie - Crocodile Dundee.\"   \n",
       "1              80  We're consuming 50 percent of the world's coca...   \n",
       "2             129   That answer was about as clear as Boston harbor.   \n",
       "3             131                          Let me help the governor.   \n",
       "4             172  We've run up more debt in the last eight years...   \n",
       "...           ...                                                ...   \n",
       "1027        28958  He has promised a trillion dollars out of the ...   \n",
       "1028        28965  (LAUGHTER) I -- there's an old high school deb...   \n",
       "1029        29011                           Well, can I answer that?   \n",
       "1030        29060  I look forward to the final weeks of this camp...   \n",
       "1031        29062     For those of you for me, thanks for your help.   \n",
       "\n",
       "     class_label  hashtags  mentions  text_length text_length_category  \\\n",
       "0             No       NaN       NaN           46                41-50   \n",
       "1            Yes       NaN       NaN           50                41-50   \n",
       "2             No       NaN       NaN           48                41-50   \n",
       "3             No       NaN       NaN           25                21-30   \n",
       "4            Yes       NaN       NaN          125                 100+   \n",
       "...          ...       ...       ...          ...                  ...   \n",
       "1027         Yes       NaN       NaN          130                 100+   \n",
       "1028          No       NaN       NaN          131                 100+   \n",
       "1029          No       NaN       NaN           26                21-30   \n",
       "1030          No       NaN       NaN           51                51-60   \n",
       "1031          No       NaN       NaN           48                41-50   \n",
       "\n",
       "      class_label_encoded  hashtags_frequency  hashtags_sentiment  \\\n",
       "0                       0                   0                   0   \n",
       "1                       1                   0                   0   \n",
       "2                       0                   0                   0   \n",
       "3                       0                   0                   0   \n",
       "4                       1                   0                   0   \n",
       "...                   ...                 ...                 ...   \n",
       "1027                    1                   0                   0   \n",
       "1028                    0                   0                   0   \n",
       "1029                    0                   0                   0   \n",
       "1030                    0                   0                   0   \n",
       "1031                    0                   0                   0   \n",
       "\n",
       "      hashtags_topics  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "1027                0  \n",
       "1028                0  \n",
       "1029                0  \n",
       "1030                0  \n",
       "1031                0  \n",
       "\n",
       "[1032 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Index(['Sentence_id', 'tweet_text', 'class_label', 'text_length', 'hashtags',\n",
       "        'mentions', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object'),\n",
       " Index(['tweet_id', 'tweet_text', 'class_label', 'text_length', 'hashtags',\n",
       "        'mentions', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object'),\n",
       " Index(['tweet_id', 'tweet_text', 'class_label', 'text_length', 'hashtags',\n",
       "        'mentions', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object'),\n",
       " Index(['tweet_id', 'tweet_text', 'class_label', 'text_length', 'hashtags',\n",
       "        'mentions', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object'),\n",
       " Index(['Sentence_id', 'tweet_text', 'class_label', 'hashtags', 'mentions',\n",
       "        'text_length', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object'),\n",
       " Index(['tweet_id', 'tweet_text', 'class_label', 'hashtags', 'mentions',\n",
       "        'text_length', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object'),\n",
       " Index(['tweet_id', 'tweet_text', 'class_label', 'text_length', 'hashtags',\n",
       "        'mentions', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object'),\n",
       " Index(['tweet_id', 'tweet_text', 'class_label', 'text_length', 'hashtags',\n",
       "        'mentions', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object'),\n",
       " Index(['Sentence_id', 'tweet_text', 'class_label', 'hashtags', 'mentions',\n",
       "        'text_length', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object'),\n",
       " Index(['tweet_id', 'tweet_text', 'class_label', 'hashtags', 'mentions',\n",
       "        'text_length', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object'),\n",
       " Index(['tweet_id', 'tweet_text', 'class_label', 'text_length', 'hashtags',\n",
       "        'mentions', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object'),\n",
       " Index(['tweet_id', 'tweet_text', 'class_label', 'text_length', 'hashtags',\n",
       "        'mentions', 'text_length_category', 'class_label_encoded',\n",
       "        'hashtags_frequency', 'hashtags_sentiment', 'hashtags_topics'],\n",
       "       dtype='object')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    en_train.columns,\n",
    "    du_train.columns,\n",
    "    es_train.columns,\n",
    "    ar_train.columns,\n",
    "    en_test.columns,\n",
    "    du_test.columns,\n",
    "    es_test.columns,\n",
    "    ar_test.columns,\n",
    "    en_dev_test.columns,\n",
    "    du_dev_test.columns,\n",
    "    es_dev_test.columns,\n",
    "    ar_dev_test.columns,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = [en_train, du_train, es_train, ar_train]\n",
    "tests = [en_test, du_test, es_test, ar_test]\n",
    "dev_tests = [en_dev_test, du_dev_test, es_dev_test, ar_dev_test]\n",
    "\n",
    "\n",
    "cols_to_keep = [\n",
    "    \"tweet_text\",\n",
    "    \"Text\",\n",
    "    \"class_label\",\n",
    "    \"hashtags\",\n",
    "    \"mentions\",\n",
    "    \"text_length\",\n",
    "    \"text_length_category\",\n",
    "    \"hashtags_frequency\",\n",
    "    \"hashtags_sentiment\",\n",
    "    \"hashtags_topics\",\n",
    "]\n",
    "\n",
    "for train in [\n",
    "    en_train,\n",
    "    du_train,\n",
    "    es_train,\n",
    "    ar_train,\n",
    "    en_test,\n",
    "    du_test,\n",
    "    es_test,\n",
    "    ar_test,\n",
    "    en_dev_test,\n",
    "    du_dev_test,\n",
    "    es_dev_test,\n",
    "    ar_dev_test,\n",
    "]:\n",
    "    cols_to_drop = [col for col in train.columns if col not in cols_to_keep]\n",
    "    train.drop(columns=cols_to_drop, inplace=True)\n",
    "    train.rename(\n",
    "        columns={\"Text\": \"text\", \"class_label\": \"label\", \"tweet_text\": \"text\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "merged_train = pd.concat(trains).sample(frac=1).reset_index(drop=True)\n",
    "merged_test = pd.concat(tests).sample(frac=1).reset_index(drop=True)\n",
    "merged_dev_test = pd.concat(dev_tests).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "merged_train[\"label\"] = merged_train[\"label\"].apply(lambda x: 1 if x == \"Yes\" else 0)\n",
    "merged_test[\"label\"] = merged_test[\"label\"].apply(lambda x: 1 if x == \"Yes\" else 0)\n",
    "merged_dev_test[\"label\"] = merged_dev_test[\"label\"].apply(\n",
    "    lambda x: 1 if x == \"Yes\" else 0\n",
    ")\n",
    "\n",
    "merged_train.to_csv(\"../data/processed/merged_train.tsv\", sep=\"\\t\", index=False)\n",
    "merged_test.to_csv(\"../data/processed/merged_test.tsv\", sep=\"\\t\", index=False)\n",
    "merged_dev_test.to_csv(\"../data/processed/merged_dev_test.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
