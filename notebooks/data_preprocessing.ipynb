{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LhP-vtTx3pC"
      },
      "source": [
        "# **Data Processing**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eo4HPuTfaEXn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import csv\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xam0IaWOxeW"
      },
      "source": [
        "**Reading tsv and fixing collisions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qAzUHhElLaiz"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>text</th>\n",
              "      <th>class_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30313</td>\n",
              "      <td>And so I know that this campaign has caused so...</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19099</td>\n",
              "      <td>\"Now, let's balance the budget and protect Med...</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33964</td>\n",
              "      <td>I'd like to mention one thing.</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16871</td>\n",
              "      <td>I must remind him the Democrats have controlle...</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13150</td>\n",
              "      <td>\"And to take a chance uh - now be - and not ma...</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentence_id                                               text class_label\n",
              "0       30313  And so I know that this campaign has caused so...          No\n",
              "1       19099  \"Now, let's balance the budget and protect Med...          No\n",
              "2       33964                     I'd like to mention one thing.          No\n",
              "3       16871  I must remind him the Democrats have controlle...         Yes\n",
              "4       13150  \"And to take a chance uh - now be - and not ma...          No"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def open_tsv_file(filename):\n",
        "    rows = []\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            fields = line.strip().split(\"\\t\")\n",
        "            # Check if the line contains more than three fields (indicating a combined line)\n",
        "            if len(fields) > 3:\n",
        "                match = re.match(r\"(.*?)(Yes|No)(\\d+)\", fields[2])\n",
        "                if match:\n",
        "                    class_label = match.group(2)\n",
        "                    next_sentence_id = match.group(3)\n",
        "                    # Append two separate rows with corrected structure\n",
        "                    rows.append([fields[0], fields[1], class_label])\n",
        "                    rows.append([next_sentence_id, fields[3], fields[4]])\n",
        "            else:\n",
        "                rows.append(fields)\n",
        "    return rows\n",
        "\n",
        "\n",
        "rows = open_tsv_file(\"../data/raw/CT24_checkworthy_english_train.tsv\")\n",
        "train_df = pd.DataFrame(rows[1:], columns=[\"sentence_id\", \"text\", \"class_label\"])\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaYD3AMCp30W"
      },
      "source": [
        "# Data exploration\n",
        "\n",
        "\n",
        "\n",
        "*   Check for any missing values.\n",
        "*   Examine the distribution of class labels to see if there's class imbalance\n",
        "*   Explore the length of the text data to understand its distribution.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qqLWYs-tqMyu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values:\n",
            " sentence_id    0\n",
            "text           0\n",
            "class_label    0\n",
            "dtype: int64\n",
            "\n",
            "Class distribution:\n",
            " class_label\n",
            "No     0.759433\n",
            "Yes    0.240567\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Text length statistics:\n",
            "Mean: 17.943602506555266\n",
            "Median: 14.0\n",
            "Minimum: 1\n",
            "Maximum: 152\n"
          ]
        }
      ],
      "source": [
        "def data_exploration(train_df):\n",
        "    # Check for any missing values\n",
        "    missing_values = train_df.isnull().sum()\n",
        "    print(\"Missing values:\\n\", missing_values)\n",
        "\n",
        "    # Examine the distribution of class labels\n",
        "    class_distribution = train_df[\"class_label\"].value_counts(normalize=True)\n",
        "    print(\"\\nClass distribution:\\n\", class_distribution)\n",
        "\n",
        "    # Explore the length of the text data\n",
        "    text_length = train_df[\"text\"].apply(lambda x: len(x.split()))\n",
        "    print(\"\\nText length statistics:\")\n",
        "    print(\"Mean:\", text_length.mean())\n",
        "    print(\"Median:\", text_length.median())\n",
        "    print(\"Minimum:\", text_length.min())\n",
        "    print(\"Maximum:\", text_length.max())\n",
        "    return\n",
        "\n",
        "\n",
        "data_exploration(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "4R-GnM9Lm3WW"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title class_label\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "train_df.groupby(\"class_label\").size().plot(\n",
        "    kind=\"barh\", color=sns.palettes.mpl_palette(\"Dark2\")\n",
        ")\n",
        "plt.gca().spines[\n",
        "    [\n",
        "        \"top\",\n",
        "        \"right\",\n",
        "    ]\n",
        "].set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrDW0RK83Irn"
      },
      "source": [
        "# **Essential preprocessing**\n",
        "1.   Normalization (additionally: Named Entity Recognition (NER),Part-of-Speech (POS) Taggin)  \n",
        "2.   Tokenization (XLM-RoBERTa tokenizer)\n",
        "3.   Saving the processed train data for further use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw5D5LqCKja9"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "  Once the text is normalized, adding feature engineering techniques to enhance the dataset.    \n",
        "  \n",
        "  Word Count: It counts the number of words in each text.\n",
        "  \n",
        "  Sentiment Polarity: It measures the sentiment polarity (positive, negative, or neutral) of the text using TextBlob.\n",
        "  \n",
        "  Subjectivity: It measures the subjectivity (opinionated vs. factual) of the text using TextBlob.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "97LoxuNU7f0n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  sentence_id                                               text class_label  \\\n",
            "0       30313  and so I know that this campaign have cause so...          No   \n",
            "1       19099  now let be balance the budget and protect Medi...          No   \n",
            "2       33964                  I would like to mention one thing          No   \n",
            "3       16871  I must remind he the Democrats have control th...         Yes   \n",
            "4       13150  and to take a chance uh   now be   and not mak...          No   \n",
            "\n",
            "   sentiment  subjectivity  word_count  \n",
            "0        0.5      0.500000          22  \n",
            "1        0.0      0.000000          14  \n",
            "2        0.0      0.000000           7  \n",
            "3        0.0      0.066667          22  \n",
            "4        0.8      0.750000          32  \n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Load the spaCy model for lemmatization and named entity recognition (NER)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Pre-compile regular expressions\n",
        "contractions = {\n",
        "    r\"n't\": \" not\",\n",
        "    r\"'s\": \" is\",\n",
        "    r\"'m\": \" am\",\n",
        "    r\"'re\": \" are\",\n",
        "    r\"'ll\": \" will\",\n",
        "    r\"'ve\": \" have\",\n",
        "    r\"'d\": \" would\",\n",
        "}\n",
        "contractions_re = re.compile(r\"|\".join(contractions.keys()))\n",
        "\n",
        "special_chars_re = re.compile(r\"[^a-zA-Z0-9\\s]\")\n",
        "\n",
        "# Custom stopword list\n",
        "custom_stopwords = {\"the\", \"and\", \"to\", \"of\", \"in\", \"for\", \"a\", \"on\", \"is\", \"at\", \"it\"}\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Handle contractions and possessive forms\n",
        "    text = contractions_re.sub(lambda m: contractions[m.group(0)], text)\n",
        "\n",
        "    # Remove special characters except letters, numbers, and spaces\n",
        "    text = special_chars_re.sub(\"\", text)\n",
        "\n",
        "    # Perform lemmatization\n",
        "    doc = nlp(text)\n",
        "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "    return lemmatized_text\n",
        "\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "\n",
        "def compute_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity\n",
        "\n",
        "\n",
        "def compute_subjectivity(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.subjectivity\n",
        "\n",
        "\n",
        "def full_feature_engineering(df):\n",
        "    # Apply normalization\n",
        "    df[\"text\"] = df[\"text\"].apply(normalize_text)\n",
        "\n",
        "    # Batch processing for sentiment analysis\n",
        "    df[\"sentiment\"] = df[\"text\"].apply(compute_sentiment)\n",
        "    df[\"subjectivity\"] = df[\"text\"].apply(compute_subjectivity)\n",
        "\n",
        "    # Count words\n",
        "    df[\"word_count\"] = df[\"text\"].apply(count_words)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Apply feature engineering\n",
        "train_df = full_feature_engineering(train_df)\n",
        "\n",
        "# Save preprocessed data\n",
        "train_df.to_csv(\"../data/processed/preprocessed_train.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZCU1Biy-hQ"
      },
      "source": [
        "# **Preprocessing for Evaluation**\n",
        "\n",
        "For other files (dev.tsv and dev-test.tsv), since they contain only two columns (sentence_id and text).\n",
        "\n",
        "*   Preprocess the dev_df and dev_test_df dataframes using the existing functions.\n",
        "*   Save the processed data into separate TSV files (processed_dev.tsv and processed_dev_test.tsv) for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L8jmy364zSlL"
      },
      "outputs": [],
      "source": [
        "def preprocess_texts_batch(texts):\n",
        "    # Normalize text\n",
        "    normalized_texts = [normalize_text(text) for text in texts]\n",
        "    return normalized_texts\n",
        "\n",
        "\n",
        "def preprocess_dev_data(df):\n",
        "    # Batch processing for normalization\n",
        "    df[\"text\"] = preprocess_texts_batch(df[\"text\"])\n",
        "\n",
        "    # Feature engineering\n",
        "    full_feature_engineering(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "def save_processed_dev_data(df, filepath):\n",
        "    df.to_csv(filepath, sep=\"\\t\", index=False)\n",
        "\n",
        "\n",
        "# Load dev.tsv file\n",
        "dev_df = pd.read_csv(\"../data/raw/CT24_checkworthy_english_dev.tsv\", sep=\"\\t\")\n",
        "dev_df.columns = map(str.lower, dev_df.columns)\n",
        "\n",
        "# Load dev-test.tsv file\n",
        "dev_test_df = pd.read_csv(\"../data/raw/CT24_checkworthy_english_dev-test.tsv\", sep=\"\\t\")\n",
        "dev_test_df.columns = map(str.lower, dev_test_df.columns)\n",
        "\n",
        "# Preprocessing dev_df\n",
        "processed_dev_data = preprocess_dev_data(dev_df)\n",
        "save_processed_dev_data(processed_dev_data, \"../data/processed/processed_dev.tsv\")\n",
        "\n",
        "# Preprocessing dev_test_df\n",
        "processed_dev_test_data = preprocess_dev_data(dev_test_df)\n",
        "save_processed_dev_data(\n",
        "    processed_dev_test_data, \"../data/processed/processed_dev_test.tsv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOwf-XGCJlgu"
      },
      "source": [
        "# **Tokenization for XML-RoBERTa-Large**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E-sN6mKBJlG2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>text</th>\n",
              "      <th>class_label</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>word_count</th>\n",
              "      <th>tokenized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30313</td>\n",
              "      <td>and so I know that this campaign have cause so...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>22</td>\n",
              "      <td>[input_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19099</td>\n",
              "      <td>now let be balance the budget and protect Medi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14</td>\n",
              "      <td>[input_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33964</td>\n",
              "      <td>I would like to mention one thing</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7</td>\n",
              "      <td>[input_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16871</td>\n",
              "      <td>I must remind he the Democrats have control th...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>22</td>\n",
              "      <td>[input_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13150</td>\n",
              "      <td>and to take a chance uh   now be   and not mak...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>32</td>\n",
              "      <td>[input_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22496</th>\n",
              "      <td>29631</td>\n",
              "      <td>it would be squander too believe I</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7</td>\n",
              "      <td>[input_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22497</th>\n",
              "      <td>7136</td>\n",
              "      <td>we be not allow to vote on it</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>[input_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22498</th>\n",
              "      <td>181</td>\n",
              "      <td>More Americans at work today than any time in ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>17</td>\n",
              "      <td>[input_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22499</th>\n",
              "      <td>12863</td>\n",
              "      <td>we indicate at that time that we be not go to ...</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>13</td>\n",
              "      <td>[input_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22500</th>\n",
              "      <td>11400</td>\n",
              "      <td>peace in the Middle East be in our nation be i...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11</td>\n",
              "      <td>[input_ids, attention_mask]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22501 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      sentence_id                                               text  \\\n",
              "0           30313  and so I know that this campaign have cause so...   \n",
              "1           19099  now let be balance the budget and protect Medi...   \n",
              "2           33964                  I would like to mention one thing   \n",
              "3           16871  I must remind he the Democrats have control th...   \n",
              "4           13150  and to take a chance uh   now be   and not mak...   \n",
              "...           ...                                                ...   \n",
              "22496       29631                 it would be squander too believe I   \n",
              "22497        7136                      we be not allow to vote on it   \n",
              "22498         181  More Americans at work today than any time in ...   \n",
              "22499       12863  we indicate at that time that we be not go to ...   \n",
              "22500       11400  peace in the Middle East be in our nation be i...   \n",
              "\n",
              "       class_label  sentiment  subjectivity  word_count  \\\n",
              "0                0       0.50      0.500000          22   \n",
              "1                0       0.00      0.000000          14   \n",
              "2                0       0.00      0.000000           7   \n",
              "3                1       0.00      0.066667          22   \n",
              "4                0       0.80      0.750000          32   \n",
              "...            ...        ...           ...         ...   \n",
              "22496            0       0.00      0.000000           7   \n",
              "22497            1       0.00      0.000000           8   \n",
              "22498            1       0.65      0.625000          17   \n",
              "22499            1      -0.10      0.100000          13   \n",
              "22500            0       0.00      0.000000          11   \n",
              "\n",
              "                    tokenized_text  \n",
              "0      [input_ids, attention_mask]  \n",
              "1      [input_ids, attention_mask]  \n",
              "2      [input_ids, attention_mask]  \n",
              "3      [input_ids, attention_mask]  \n",
              "4      [input_ids, attention_mask]  \n",
              "...                            ...  \n",
              "22496  [input_ids, attention_mask]  \n",
              "22497  [input_ids, attention_mask]  \n",
              "22498  [input_ids, attention_mask]  \n",
              "22499  [input_ids, attention_mask]  \n",
              "22500  [input_ids, attention_mask]  \n",
              "\n",
              "[22501 rows x 7 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import XLMRobertaTokenizer\n",
        "\n",
        "# Initialize the XLM-RoBERTa tokenizer\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
        "\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokenized_text = tokenizer(text, truncation=True, padding=\"max_length\", return_tensors=\"pt\", max_length=128)\n",
        "    return tokenized_text\n",
        "\n",
        "\n",
        "# Tokenization for XLM-RoBERTa-Large\n",
        "train_df[\"tokenized_text\"] = train_df[\"text\"].apply(tokenize_text)\n",
        "dev_df[\"tokenized_text\"] = dev_df[\"text\"].apply(tokenize_text)\n",
        "dev_test_df[\"tokenized_text\"] = dev_test_df[\"text\"].apply(tokenize_text)\n",
        "\n",
        "train_df[\"class_label\"] = train_df[\"class_label\"].apply(\n",
        "    lambda x: 1 if x == \"Yes\" else 0\n",
        ")\n",
        "dev_df[\"class_label\"] = dev_df[\"class_label\"].apply(lambda x: 1 if x == \"Yes\" else 0)\n",
        "dev_test_df[\"class_label\"] = dev_test_df[\"class_label\"].apply(\n",
        "    lambda x: 1 if x == \"Yes\" else 0\n",
        ")\n",
        "\n",
        "\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aRrOBXTGOVIy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def convert_to_lists(tokenized_text):\n",
        "    return {key: value.tolist() for key, value in tokenized_text.items()}\n",
        "\n",
        "\n",
        "# Convert tokenized data to lists\n",
        "train_df[\"tokenized_text\"] = train_df[\"tokenized_text\"].apply(convert_to_lists)\n",
        "dev_df[\"tokenized_text\"] = dev_df[\"tokenized_text\"].apply(convert_to_lists)\n",
        "dev_test_df[\"tokenized_text\"] = dev_test_df[\"tokenized_text\"].apply(convert_to_lists)\n",
        "\n",
        "train_tokenized_lists = train_df[\"tokenized_text\"]\n",
        "dev_tokenized_lists = dev_df[\"tokenized_text\"]\n",
        "dev_test_tokenized_lists = dev_test_df[\"tokenized_text\"]\n",
        "\n",
        "# Save tokenized data to JSON files\n",
        "train_tokenized_lists.to_json(\n",
        "    \"../data/tokenized/train_tokenized.json\", orient=\"records\", lines=True\n",
        ")\n",
        "dev_tokenized_lists.to_json(\n",
        "    \"../data/tokenized/dev_tokenized.json\", orient=\"records\", lines=True\n",
        ")\n",
        "dev_test_tokenized_lists.to_json(\n",
        "    \"../data/tokenized/dev_test_tokenized.json\", orient=\"records\", lines=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_ids_and_mask(row):\n",
        "    row[\"input_ids\"] = row[\"tokenized_text\"][\"input_ids\"]\n",
        "    row[\"attention_mask\"] = row[\"tokenized_text\"][\"attention_mask\"]\n",
        "    del row[\"tokenized_text\"]\n",
        "    return row\n",
        "\n",
        "\n",
        "train_df = train_df.apply(add_ids_and_mask, axis=1)\n",
        "dev_df = dev_df.apply(add_ids_and_mask, axis=1)\n",
        "dev_test_df = dev_test_df.apply(add_ids_and_mask, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.to_csv(\"../data/processed/processed_train_complete.csv\")\n",
        "dev_df.to_csv(\"../data/processed/processed_dev_complete.csv\")\n",
        "dev_test_df.to_csv(\"../data/processed/processed_dev_test_complete.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
