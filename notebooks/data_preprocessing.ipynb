{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing**\n"
      ],
      "metadata": {
        "id": "4LhP-vtTx3pC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo4HPuTfaEXn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import csv\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading tsv and fixing collisions**"
      ],
      "metadata": {
        "id": "1Xam0IaWOxeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def open_tsv_file(filename):\n",
        "    rows = []\n",
        "    with open(filename, 'r') as file:\n",
        "      for line in file:\n",
        "          fields = line.strip().split('\\t')\n",
        "          # Check if the line contains more than three fields (indicating a combined line)\n",
        "          if len(fields) > 3:\n",
        "              match = re.match(r'(.*?)(Yes|No)(\\d+)', fields[2])\n",
        "              if match:\n",
        "                  class_label = match.group(2)\n",
        "                  next_sentence_id = match.group(3)\n",
        "                  # Append two separate rows with corrected structure\n",
        "                  rows.append([fields[0], fields[1], class_label])\n",
        "                  rows.append([next_sentence_id, fields[3], fields[4]])\n",
        "          else:\n",
        "              rows.append(fields)\n",
        "    return rows\n",
        "\n",
        "rows = open_tsv_file('CT24_checkworthy_english_train.tsv')\n",
        "train_df = pd.DataFrame(rows[1:], columns=['sentence_id', 'text', 'class_label'])\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "qAzUHhElLaiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data exploration\n",
        "\n",
        "\n",
        "\n",
        "*   Check for any missing values.\n",
        "*   Examine the distribution of class labels to see if there's class imbalance\n",
        "*   Explore the length of the text data to understand its distribution.  \n",
        "\n"
      ],
      "metadata": {
        "id": "FaYD3AMCp30W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_exploration(train_df):\n",
        "  # Check for any missing values\n",
        "  missing_values = train_df.isnull().sum()\n",
        "  print(\"Missing values:\\n\", missing_values)\n",
        "\n",
        "  # Examine the distribution of class labels\n",
        "  class_distribution = train_df['class_label'].value_counts(normalize=True)\n",
        "  print(\"\\nClass distribution:\\n\", class_distribution)\n",
        "\n",
        "  # Explore the length of the text data\n",
        "  text_length = train_df['text'].apply(lambda x: len(x.split()))\n",
        "  print(\"\\nText length statistics:\")\n",
        "  print(\"Mean:\", text_length.mean())\n",
        "  print(\"Median:\", text_length.median())\n",
        "  print(\"Minimum:\", text_length.min())\n",
        "  print(\"Maximum:\", text_length.max())\n",
        "  return\n",
        "\n",
        "data_exploration(train_df)\n"
      ],
      "metadata": {
        "id": "qqLWYs-tqMyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# @title class_label\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "train_df.groupby('class_label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "4R-GnM9Lm3WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Essential preprocessing**\n",
        "1.   Normalization (additionally: Named Entity Recognition (NER),Part-of-Speech (POS) Taggin)  \n",
        "2.   Tokenization (XLM-RoBERTa tokenizer)\n",
        "3.   Saving the processed train data for further use"
      ],
      "metadata": {
        "id": "OrDW0RK83Irn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "  Once the text is normalized, adding feature engineering techniques to enhance the dataset.    \n",
        "  \n",
        "  Word Count: It counts the number of words in each text.\n",
        "  \n",
        "  Sentiment Polarity: It measures the sentiment polarity (positive, negative, or neutral) of the text using TextBlob.\n",
        "  \n",
        "  Subjectivity: It measures the subjectivity (opinionated vs. factual) of the text using TextBlob.\n"
      ],
      "metadata": {
        "id": "Xw5D5LqCKja9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Load the spaCy model for lemmatization and named entity recognition (NER)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Pre-compile regular expressions\n",
        "contractions = {\n",
        "    r\"n't\": \" not\",\n",
        "    r\"'s\": \" is\",\n",
        "    r\"'m\": \" am\",\n",
        "    r\"'re\": \" are\",\n",
        "    r\"'ll\": \" will\",\n",
        "    r\"'ve\": \" have\",\n",
        "    r\"'d\": \" would\"\n",
        "}\n",
        "contractions_re = re.compile(r\"|\".join(contractions.keys()))\n",
        "\n",
        "special_chars_re = re.compile(r'[^a-zA-Z0-9\\s]')\n",
        "\n",
        "# Custom stopword list\n",
        "custom_stopwords = {'the', 'and', 'to', 'of', 'in', 'for', 'a', 'on', 'is', 'at', 'it'}\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Handle contractions and possessive forms\n",
        "    text = contractions_re.sub(lambda m: contractions[m.group(0)], text)\n",
        "\n",
        "    # Remove special characters except letters, numbers, and spaces\n",
        "    text = special_chars_re.sub('', text)\n",
        "\n",
        "    # Perform lemmatization\n",
        "    doc = nlp(text)\n",
        "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
        "\n",
        "    return lemmatized_text\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def compute_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity\n",
        "\n",
        "def compute_subjectivity(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.subjectivity\n",
        "\n",
        "def full_feature_engineering(df):\n",
        "    # Apply normalization\n",
        "    df['text'] = df['text'].apply(normalize_text)\n",
        "\n",
        "    # Batch processing for sentiment analysis\n",
        "    df['sentiment'] = df['text'].apply(compute_sentiment)\n",
        "    df['subjectivity'] = df['text'].apply(compute_subjectivity)\n",
        "\n",
        "    # Count words\n",
        "    df['word_count'] = df['text'].apply(count_words)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering\n",
        "train_df = full_feature_engineering(train_df)\n",
        "\n",
        "# Save preprocessed data\n",
        "train_df.to_csv(\"preprocessed_train.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "97LoxuNU7f0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing for Evaluation**\n",
        "\n",
        "For other files (dev.tsv and dev-test.tsv), since they contain only two columns (sentence_id and text).\n",
        "\n",
        "*   Preprocess the dev_df and dev_test_df dataframes using the existing functions.\n",
        "*   Save the processed data into separate TSV files (processed_dev.tsv and processed_dev_test.tsv) for evaluation."
      ],
      "metadata": {
        "id": "XTZCU1Biy-hQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_texts_batch(texts):\n",
        "    # Normalize text\n",
        "    normalized_texts = [normalize_text(text) for text in texts]\n",
        "    return normalized_texts\n",
        "\n",
        "def preprocess_dev_data(df):\n",
        "    # Batch processing for normalization\n",
        "    df['text'] = preprocess_texts_batch(df['text'])\n",
        "\n",
        "    # Feature engineering\n",
        "    full_feature_engineering(df)\n",
        "    return df\n",
        "\n",
        "def save_processed_dev_data(df, filepath):\n",
        "    df.to_csv(filepath, sep='\\t', index=False)\n",
        "\n",
        "# Load dev.tsv file\n",
        "dev_df = pd.read_csv('CT24_checkworthy_english_dev.tsv', sep='\\t')\n",
        "dev_df.columns = map(str.lower, dev_df.columns)\n",
        "\n",
        "# Load dev-test.tsv file\n",
        "dev_test_df = pd.read_csv('CT24_checkworthy_english_dev-test.tsv', sep='\\t')\n",
        "dev_test_df.columns = map(str.lower, dev_test_df.columns)\n",
        "\n",
        "# Preprocessing dev_df\n",
        "processed_dev_data = preprocess_dev_data(dev_df)\n",
        "save_processed_dev_data(processed_dev_data, 'processed_dev.tsv')\n",
        "\n",
        "# Preprocessing dev_test_df\n",
        "processed_dev_test_data = preprocess_dev_data(dev_test_df)\n",
        "save_processed_dev_data(processed_dev_test_data, 'processed_dev_test.tsv')\n"
      ],
      "metadata": {
        "id": "L8jmy364zSlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization for XML-RoBERTa-Large**"
      ],
      "metadata": {
        "id": "wOwf-XGCJlgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer\n",
        "\n",
        "# Initialize the XLM-RoBERTa tokenizer\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokenized_text = tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n",
        "    return tokenized_text\n",
        "\n",
        "# Tokenization for XLM-RoBERTa-Large\n",
        "train_tokenized = train_df['text'].apply(tokenize_text)\n",
        "dev_tokenized = dev_df['text'].apply(tokenize_text)\n",
        "dev_test_tokenized = dev_test_df['text'].apply(tokenize_text)"
      ],
      "metadata": {
        "id": "E-sN6mKBJlG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def convert_to_lists(tokenized_text):\n",
        "    return {key: value.tolist() for key, value in tokenized_text.items()}\n",
        "\n",
        "# Convert tokenized data to lists\n",
        "train_tokenized_lists = train_tokenized.apply(convert_to_lists)\n",
        "dev_tokenized_lists = dev_tokenized.apply(convert_to_lists)\n",
        "dev_test_tokenized_lists = dev_test_tokenized.apply(convert_to_lists)\n",
        "\n",
        "# Save tokenized data to JSON files\n",
        "train_tokenized_lists.to_json(\"train_tokenized.json\", orient=\"records\", lines=True)\n",
        "dev_tokenized_lists.to_json(\"dev_tokenized.json\", orient=\"records\", lines=True)\n",
        "dev_test_tokenized_lists.to_json(\"dev_test_tokenized.json\", orient=\"records\", lines=True)"
      ],
      "metadata": {
        "id": "aRrOBXTGOVIy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}